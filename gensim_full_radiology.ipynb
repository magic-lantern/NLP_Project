{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# gensim docs recommend setting this up, but the following doesn't work in Jupyter\n",
    "##logger = logging.getLogger(__name__)\n",
    "##logger.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# found that this works - though output is to console, not Jupyter output\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 522279 documents.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "#sqlitedb = os.path.join(os.path.expanduser('~'),'Box Sync', 'GradSchoolStuff', 'MastersProject', 'ctpa.sqlite')\n",
    "sqlitedb = os.path.join(os.path.expanduser('~'),'Box Sync', 'GradSchoolStuff', 'MastersProject', 'mimic3', 'mimic3.sqlite')\n",
    "if not (os.path.exists(sqlitedb)):\n",
    "    print(\"Specified database does not exist\")\n",
    "    sys.exit()\n",
    "\n",
    "connection = sqlite3.connect(sqlitedb)\n",
    "with connection:\n",
    "    cur = connection.cursor()\n",
    "#    cur.execute('select * from reports')\n",
    "    cur.execute(\"select text from noteevents where category = 'Radiology'\")\n",
    "#    col_names = [cn[0] for cn in cur.description]\n",
    "    rows = cur.fetchall()\n",
    "    #print(len(rows[0]))\n",
    "    #print(\"%s %s %s %s %s %s\" % (col_names[0], col_names[1], col_names[2], col_names[3], col_names[4], col_names[5]))\n",
    "\n",
    "    documents = []\n",
    "    for row in rows:\n",
    "#        d = row[4]\n",
    "        documents.append(row[0])\n",
    "    print('Read', len(documents), 'documents.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 522279 (should agree with previous number of documents.)\n",
      "Total sentences: 8721909\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "counter = 0\n",
    "training_sentences = []\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# really need to parallelize this - silly to only process one document at a time!\n",
    "for document in documents:\n",
    "    # convert lines of underscores into period so they trigger a sentence boundary with NLTK\n",
    "    document = re.sub( '___+', '.', document)\n",
    "\n",
    "    counter += 1\n",
    "    # Load the punkt tokenizer pre-trained on english text to improve\n",
    "    # sentence splitting, would need to create custom tokenizer that understands\n",
    "    # radiology report sections. However, I think this may be good enoug for now.\n",
    "    output = sent_tokenizer.tokenize(document)\n",
    "    \n",
    "    # NLTK sentence splitter; handles punctuation better, but don't like how\n",
    "    # \"we'll\" becomes two words \"we\" and \"'ll\"\n",
    "    output = [word_tokenize(o.lower()) for o in output]    \n",
    "    # alternative std python split function - this is much faster than the NLTK splitter\n",
    "    #output = [o.lower().split() for o in output]\n",
    "\n",
    "    if (counter % 10000 == 0):\n",
    "        logger.info('Processed ' + str(counter) + ' documents.')\n",
    "\n",
    "    for o in output:\n",
    "        training_sentences.append(o)\n",
    "    \n",
    "#pp.pprint(training_sentences)\n",
    "print('Total documents:', counter, '(should agree with previous number of documents.)')\n",
    "print('Total sentences:', len(training_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready for use.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec, Phrases\n",
    "retrain = True\n",
    "if(retrain):\n",
    "    # Set values for various parameters, starting point provided by \n",
    "    # https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "    num_features = 300    # Word vector dimensionality                      \n",
    "    min_word_count = 10   # Minimum word count, default is 5\n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 20          # Context window size - set to large as some report sections \n",
    "                          #    aren't prose but are instead mostly shorthand notation  \n",
    "    # default sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "    # from the gensim documentation:\n",
    "    #   Note that there is a gensim.models.phrases module which lets you automatically detect\n",
    "    #    phrases longer than one word. Using phrases, you can learn a word2vec model where \n",
    "    #    “words” are actually multiword expressions, such as new_york_times or financial_crisis:\n",
    "    bigram_transformer = Phrases(training_sentences)\n",
    "    trigram_transformer = Phrases(bigram_transformer[training_sentences])\n",
    "    #model = Word2Vec(bigram_transformer[sentences], size=100, ...)\n",
    "\n",
    "    model = word2vec.Word2Vec(trigram_transformer[bigram_transformer[training_sentences]], \\\n",
    "                              workers=num_workers, \\\n",
    "                              size=num_features, \\\n",
    "                              min_count = min_word_count, \\\n",
    "                              window = context)\n",
    "    model.save(os.path.join(os.path.expanduser('~'),'Box Sync', 'GradSchoolStuff', 'MastersProject', 'mimic3', 'word2vec_full_radiology.model'))\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(os.path.join(os.path.expanduser('~'),'Box Sync', 'GradSchoolStuff', 'MastersProject', 'mimic3', 'word2vec_full_radiology.model'))\n",
    "\n",
    "print('Model ready for use.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pulmonary_embolus', 0.8787556886672974),\n",
       " ('pulmonary_emboli', 0.7133843898773193),\n",
       " ('subsegmental', 0.6346239447593689),\n",
       " ('subsegmental_branches', 0.600606381893158),\n",
       " ('pe', 0.5958702564239502)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"pulmonary_embolism\", topn=5)\n",
    "#model.most_similar(\"hypertension\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pulmonary_embolus', 0.939376950263977),\n",
       " ('pulmonary_emboli', 0.8566913604736328),\n",
       " ('subsegmental', 0.8173112273216248),\n",
       " ('subsegmental_branches', 0.8003024458885193),\n",
       " ('pe', 0.797934353351593)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar_cosmul(\"pulmonary_embolism\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fracture'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"embolism fracture pulmonary lung\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.018186284898593707"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('metatarsal', 'pulmonary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model['pulmonary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
