{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# gensim docs recommend setting this up, but the following doesn't work in Jupyter\n",
    "##logger = logging.getLogger(__name__)\n",
    "##logger.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# found that this works - though output is to console, not Jupyter output\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "#sqlitedb = os.path.join(os.path.expanduser('~'),'Box Sync', 'GradSchoolStuff', 'MastersProject', 'ctpa.sqlite')\n",
    "sqlitedb = os.path.join(os.path.expanduser('~'),'Box Sync', 'GradSchoolStuff', 'MastersProject', 'mimic3', 'mimic3.sqlite')\n",
    "if not (os.path.exists(sqlitedb)):\n",
    "    print(\"Specified database does not exist\")\n",
    "    sys.exit()\n",
    "\n",
    "connection = sqlite3.connect(sqlitedb)\n",
    "with connection:\n",
    "    cur = connection.cursor()\n",
    "#    cur.execute('select * from reports')\n",
    "    cur.execute(\"select text from noteevents where category = 'Radiology'\")\n",
    "#    col_names = [cn[0] for cn in cur.description]\n",
    "    rows = cur.fetchall()\n",
    "    #print(len(rows[0]))\n",
    "    #print(\"%s %s %s %s %s %s\" % (col_names[0], col_names[1], col_names[2], col_names[3], col_names[4], col_names[5]))\n",
    "\n",
    "    documents = []\n",
    "    for row in rows:\n",
    "#        d = row[4]\n",
    "        documents.append(row[0])\n",
    "    print('Read', len(documents), 'documents.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "counter = 0\n",
    "training_sentences = []\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# really need to parallelize this - silly to only process one document at a time!\n",
    "for document in documents:\n",
    "    # convert lines of underscores into period so they trigger a sentence boundary with NLTK\n",
    "    document = re.sub( '___+', '.', document)\n",
    "\n",
    "    counter += 1\n",
    "    # Load the punkt tokenizer pre-trained on english text to improve\n",
    "    # sentence splitting, would need to create custom tokenizer that understands\n",
    "    # radiology report sections. However, I think this may be good enoug for now.\n",
    "    output = sent_tokenizer.tokenize(document)\n",
    "    \n",
    "    # NLTK sentence splitter; handles punctuation better, but don't like how\n",
    "    # \"we'll\" becomes two words \"we\" and \"'ll\"\n",
    "    output = [word_tokenize(o.lower()) for o in output]    \n",
    "    # alternative std python split function - this is much faster than the NLTK splitter\n",
    "    #output = [o.lower().split() for o in output]\n",
    "\n",
    "    if (counter % 10000 == 0):\n",
    "        logger.info('Processed ' + str(counter) + ' documents.')\n",
    "\n",
    "    for o in output:\n",
    "        training_sentences.append(o)\n",
    "    \n",
    "#pp.pprint(training_sentences)\n",
    "print('Total documents:', counter, '(should agree with previous number of documents.)')\n",
    "print('Total sentences:', len(training_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only want 'words' that have either letters or numbers - exclude items that are only punctuation or other symbols.\n",
    "real_word = re.compile('.*\\w+.*')\n",
    "total_word_count = 0\n",
    "for s in training_sentences:\n",
    "    for w in s:\n",
    "        if real_word.match(w):\n",
    "            total_word_count += 1\n",
    "            \n",
    "print('Total words:', total_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from gensim.models import word2vec, Phrases\n",
    "retrain = False\n",
    "if(retrain):\n",
    "    # Set values for various parameters, starting point provided by \n",
    "    # https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "    size = 300            # Word vector dimensionality, previously this was \"num_features\"                      \n",
    "    min_word_count = 10   # Minimum word count, default is 5\n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 20          # Context window size - set to large as some report sections \n",
    "                          #    aren't prose but are instead mostly shorthand notation  \n",
    "    # default sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "    # from the gensim documentation:\n",
    "    #   Note that there is a gensim.models.phrases module which lets you automatically detect\n",
    "    #    phrases longer than one word. Using phrases, you can learn a word2vec model where \n",
    "    #    “words” are actually multiword expressions, such as new_york_times or financial_crisis:\n",
    "    bigram_transformer = Phrases(training_sentences)\n",
    "    trigram_transformer = Phrases(bigram_transformer[training_sentences])\n",
    "    #model = Word2Vec(bigram_transformer[sentences], size=100, ...)\n",
    "\n",
    "    model = word2vec.Word2Vec(trigram_transformer[bigram_transformer[training_sentences]], \\\n",
    "                              workers=num_workers, \\\n",
    "                              size=num_features, \\\n",
    "                              min_count = min_word_count, \\\n",
    "                              window = context)\n",
    "    model.save(os.path.join(os.path.expanduser('~'),'Box Sync', 'GradSchoolStuff', 'MastersProject', 'mimic3', 'word2vec_full_radiology.model'))\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(os.path.join(os.path.expanduser('~'),'Box Sync', 'GradSchoolStuff', 'MastersProject', 'mimic3', 'word2vec_full_radiology.model'))\n",
    "\n",
    "print('Model ready for use.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"pe\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"pulmonary_embolism\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar_cosmul(\"pe\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"embolism fracture pulmonary lung\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity('pe', 'pulmonary_embolism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "print(type(model.index2word))\n",
    "for m in model.index2word:\n",
    "    print(m)\n",
    "    count += 1\n",
    "\n",
    "print(\"Total terms in model: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
